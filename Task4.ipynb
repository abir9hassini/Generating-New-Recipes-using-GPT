{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Natural Language Processing to Generate New Recipes in Python\n",
    "## Task 4: GPT-2 Pretrained Model Formulation and Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31c9dcd692a4fc2b1d1608ef46d60bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579d86bee9f148139efb2cd476147af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3b810d331a46ad834aa634726199b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c04162efe2e4ed58beb036f90a0a6df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[28065,    11,    40,  6807,   284,   262,  6128,    11,   290]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text = 'Yesterday,I walked to the shop, and'\n",
    "encoded_prompt = tokenizer.encode(\n",
    "    prompt_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "encoded_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    }
   ],
   "source": [
    "output_sequence = model.generate(\n",
    "    input_ids = encoded_prompt,\n",
    "    max_length=700,\n",
    "    temperature=0.9,\n",
    "    top_k=20,\n",
    "    top_p=0.9,\n",
    "    repetition_penality=1,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yesterday,I walked to the shop, and then went to the front, which was where I found a few different items, which I did not know had been found. It was a box that had been opened, and I walked into the shop to check the order. I opened it to look at it and saw what the package contained, and I found something that wasn't in the box. I had to open it up and see what the box contained, and I found something that I found in my box. I found something that wasn't in the box. The box had been filled with paper, and I found something that I didn't know. I went in and opened it up and saw the box. I opened it and saw the box. It was like a box, but I found something that wasn't in the box, so I opened it up and found it. I opened it and saw it. I opened it and saw the box. I opened it and saw the box. I opened it and saw it. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw the box. I opened it and saw\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdo_sample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mearly_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_beams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbad_words_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbos_token_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpad_token_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0meos_token_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlength_penalty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mno_repeat_ngram_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mattention_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdecoder_start_token_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mmodel_specific_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Generates sequences for models with a LM head. The method currently supports greedy decoding, beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.\n",
       "\n",
       "Adapted in part from `Facebook's XLM beam search code`_.\n",
       "\n",
       ".. _`Facebook's XLM beam search code`:\n",
       "   https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529\n",
       "\n",
       "\n",
       "Parameters:\n",
       "\n",
       "    input_ids: (`optional`) `torch.LongTensor` of shape `(batch_size, sequence_length)`\n",
       "        The sequence used as a prompt for the generation. If `None` the method initializes\n",
       "        it as an empty `torch.LongTensor` of shape `(1,)`.\n",
       "\n",
       "    max_length: (`optional`) int\n",
       "        The max length of the sequence to be generated.  Between `min_length` and infinity. Default to 20.\n",
       "\n",
       "    min_length: (`optional`) int\n",
       "        The min length of the sequence to be generated.  Between 0 and infinity. Default to 0.\n",
       "\n",
       "    do_sample: (`optional`) bool\n",
       "        If set to `False` greedy decoding is used. Otherwise sampling is used. Defaults to `False` as defined in `configuration_utils.PretrainedConfig`.\n",
       "\n",
       "    early_stopping: (`optional`) bool\n",
       "        if set to `True` beam search is stopped when at least `num_beams` sentences finished per batch. Defaults to `False` as defined in `configuration_utils.PretrainedConfig`.\n",
       "\n",
       "    num_beams: (`optional`) int\n",
       "        Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1.\n",
       "\n",
       "    temperature: (`optional`) float\n",
       "        The value used to module the next token probabilities. Must be strictly positive. Default to 1.0.\n",
       "\n",
       "    top_k: (`optional`) int\n",
       "        The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Default to 50.\n",
       "\n",
       "    top_p: (`optional`) float\n",
       "        The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Must be between 0 and 1. Default to 1.\n",
       "\n",
       "    repetition_penalty: (`optional`) float\n",
       "        The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.\n",
       "\n",
       "    pad_token_id: (`optional`) int\n",
       "        Padding token. Default to specicic model pad_token_id or None if it does not exist.\n",
       "\n",
       "    bos_token_id: (`optional`) int\n",
       "        BOS token. Defaults to `bos_token_id` as defined in the models config.\n",
       "\n",
       "    eos_token_id: (`optional`) int\n",
       "        EOS token. Defaults to `eos_token_id` as defined in the models config.\n",
       "\n",
       "    length_penalty: (`optional`) float\n",
       "        Exponential penalty to the length. Default to 1.\n",
       "\n",
       "    no_repeat_ngram_size: (`optional`) int\n",
       "        If set to int > 0, all ngrams of size `no_repeat_ngram_size` can only occur once.\n",
       "    bad_words_ids: (`optional`) list of lists of int\n",
       "        `bad_words_ids` contains tokens that are not allowed to be generated. In order to get the tokens of the words that should not appear in the generated text, use `tokenizer.encode(bad_word, add_prefix_space=True)`.\n",
       "\n",
       "    num_return_sequences: (`optional`) int\n",
       "        The number of independently computed returned sequences for each element in the batch. Default to 1.\n",
       "\n",
       "    attention_mask (`optional`) obj: `torch.LongTensor` of same shape as `input_ids`\n",
       "        Mask to avoid performing attention on padding token indices.\n",
       "        Mask values selected in ``[0, 1]``:\n",
       "        ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n",
       "        Defaults to `None`.\n",
       "\n",
       "        `What are attention masks? <../glossary.html#attention-mask>`__\n",
       "\n",
       "    decoder_start_token_id=None: (`optional`) int\n",
       "        If an encoder-decoder model starts decoding with a different token than BOS.\n",
       "        Defaults to `None` and is changed to `BOS` later.\n",
       "\n",
       "    use_cache: (`optional`) bool\n",
       "        If `use_cache` is True, past key values are used to speed up decoding if applicable to model. Defaults to `True`.\n",
       "\n",
       "    model_specific_kwargs: (`optional`) dict\n",
       "        Additional model specific kwargs will be forwarded to the `forward` function of the model.\n",
       "\n",
       "Return:\n",
       "\n",
       "    output: `torch.LongTensor` of shape `(batch_size * num_return_sequences, sequence_length)`\n",
       "        sequence_length is either equal to max_length or shorter if all batches finished early due to the `eos_token_id`\n",
       "\n",
       "Examples::\n",
       "\n",
       "    tokenizer = AutoTokenizer.from_pretrained('distilgpt2')   # Initialize tokenizer\n",
       "    model = AutoModelWithLMHead.from_pretrained('distilgpt2')    # Download model and configuration from S3 and cache.\n",
       "    outputs = model.generate(max_length=40)  # do greedy decoding\n",
       "    print('Generated: {}'.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))\n",
       "\n",
       "    tokenizer = AutoTokenizer.from_pretrained('openai-gpt')   # Initialize tokenizer\n",
       "    model = AutoModelWithLMHead.from_pretrained('openai-gpt')    # Download model and configuration from S3 and cache.\n",
       "    input_context = 'The dog'\n",
       "    input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\n",
       "    outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3, temperature=1.5)  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'\n",
       "    for i in range(3): #  3 output sequences were generated\n",
       "        print('Generated {}: {}'.format(i, tokenizer.decode(outputs[i], skip_special_tokens=True)))\n",
       "\n",
       "    tokenizer = AutoTokenizer.from_pretrained('distilgpt2')   # Initialize tokenizer\n",
       "    model = AutoModelWithLMHead.from_pretrained('distilgpt2')    # Download model and configuration from S3 and cache.\n",
       "    input_context = 'The dog'\n",
       "    input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\n",
       "    outputs = model.generate(input_ids=input_ids, max_length=40, temperature=0.7, num_return_sequences=3)  # 3 generate sequences using by sampling\n",
       "    for i in range(3): #  3 output sequences were generated\n",
       "        print('Generated {}: {}'.format(i, tokenizer.decode(outputs[i], skip_special_tokens=True)))\n",
       "\n",
       "    tokenizer = AutoTokenizer.from_pretrained('ctrl')   # Initialize tokenizer\n",
       "    model = AutoModelWithLMHead.from_pretrained('ctrl')    # Download model and configuration from S3 and cache.\n",
       "    input_context = 'Legal My neighbor is'  # \"Legal\" is one of the control codes for ctrl\n",
       "    input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\n",
       "    outputs = model.generate(input_ids=input_ids, max_length=50, temperature=0.7, repetition_penalty=1.2)  # generate sequences\n",
       "    print('Generated: {}'.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))\n",
       "\n",
       "    tokenizer = AutoTokenizer.from_pretrained('gpt2')   # Initialize tokenizer\n",
       "    model = AutoModelWithLMHead.from_pretrained('gpt2')    # Download model and configuration from S3 and cache.\n",
       "    input_context = 'My cute dog'  # \"Legal\" is one of the control codes for ctrl\n",
       "    bad_words_ids = [tokenizer.encode(bad_word, add_prefix_space=True) for bad_word in ['idiot', 'stupid', 'shut up']]\n",
       "    input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\n",
       "    outputs = model.generate(input_ids=input_ids, max_length=100, do_sample=True, bad_words_ids=bad_words_ids)  # generate sequences without allowing bad_words to be generated\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.6/site-packages/transformers/generation_utils.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.generate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
